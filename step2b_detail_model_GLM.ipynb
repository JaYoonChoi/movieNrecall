{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e503936-ca9b-43a2-a5b0-b24a21cccf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load required modules\n",
    "import warnings, sys, os ## system\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\") # ignore warnings\n",
    "\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime    \n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product, combinations\n",
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "from bids import BIDSLayout, BIDSValidator\n",
    "\n",
    "from nilearn.maskers import NiftiMasker\n",
    "from nilearn import plotting, image\n",
    "from nilearn.glm.first_level import make_first_level_design_matrix, FirstLevelModel\n",
    "from nilearn.image import concat_imgs, mean_img\n",
    "from nilearn.plotting import plot_design_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load files \n",
    "story_dir = '/mnt/c/Users/since/Dropbox/Lab/Projects/CJY/_DATA'\n",
    "May_dir = '/mnt/c/Users/since/Desktop/KdramaMay/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e41abaf4-b70e-4b51-974d-124416e25bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load recall data\n",
    "import json\n",
    "import csv\n",
    "import seaborn as sns\n",
    "\n",
    "overall_performance_dir = \"/mnt/c/Users/since/Dropbox/RAs/projects\"\n",
    "\n",
    "# Set columns \n",
    "# #Subject check \n",
    "sub_sm = np.arange(1,34)\n",
    "sub_sm_exclude = [8, 25] #exclude 9 & 26\n",
    "subjects1 = np.delete(sub_sm, sub_sm_exclude)\n",
    "\n",
    "sub_idx, vis, aud, subj = [], [], [], []\n",
    "for sub in subjects1:\n",
    "    \n",
    "    if sub < 10:\n",
    "        aud_idx = 'A0'+str(sub)\n",
    "        vis_idx = 'V0'+str(sub)\n",
    "        subj_idx = 'S0'+str(sub)\n",
    "    else:\n",
    "        aud_idx = 'A'+str(sub)\n",
    "        vis_idx = 'V'+str(sub)\n",
    "        subj_idx = 'S'+str(sub)\n",
    "        (sub)\n",
    "        \n",
    "    sub_idx.append(vis_idx)\n",
    "    sub_idx.append(aud_idx)\n",
    "    vis.append(vis_idx)\n",
    "    aud.append(aud_idx)\n",
    "    subj.append(subj_idx)\n",
    "    #aud.append(aud_idx)\n",
    "    \n",
    "sub_sm2 = np.arange(34,77)\n",
    "#print(sub_sm2)\n",
    "sub_sm_exclude2 = [2, 3, 7, 13, 22, 23, 26, 27,28,29,35] #Excluede 36, 37, 41, 47, 56, 57, 60, 61,62,63,69\n",
    "subjects2 = np.delete(sub_sm2, sub_sm_exclude2)\n",
    "#print(subjects2)\n",
    "# Set columns \n",
    "sub_idx2, vis2, aud2, subj2 = [], [], [], []\n",
    "for sub in subjects2:\n",
    "    \n",
    "    if sub < 10:\n",
    "        aud_idx = 'A0'+str(sub)\n",
    "        vis_idx = 'V0'+str(sub)\n",
    "        subj_idx = 'S0'+str(sub)\n",
    "    else:\n",
    "        aud_idx = 'A'+str(sub)\n",
    "        vis_idx = 'V'+str(sub)\n",
    "        subj_idx = 'S'+str(sub)\n",
    "        \n",
    "    sub_idx2.append(vis_idx)\n",
    "    sub_idx2.append(aud_idx)\n",
    "    vis2.append(vis_idx)\n",
    "    aud2.append(aud_idx)\n",
    "    subj2.append(subj_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "912419ca-2cf0-4d91-8411-dd8fb2aaf79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_sm = np.append(subjects1, subjects2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40f6cf85-f9ca-4b8f-87f9-6c8be5597d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load story sequence \n",
    "import scipy.io as sio\n",
    "\n",
    "all_story = {}\n",
    "# Load May Data\n",
    "for sub in sub_sm:\n",
    "    #print(sub)\n",
    "    mat = sio.loadmat(glob.glob(os.path.join(story_dir, 'fMRItdsgn', f'sub-{sub}_*.mat'))[0])\n",
    "    story = []\n",
    "    for i in np.arange(0,4):\n",
    "        temp = mat['subj'][0][0][7][0][i][0]\n",
    "        if temp == 'Blind Date':\n",
    "            temp = 'BlindDate'\n",
    "        elif temp == 'Runningman':\n",
    "            temp = 'RunningMan'\n",
    "        elif temp == 'neighbors':\n",
    "            temp = 'Neighbors'\n",
    "        story = np.append(story, temp)\n",
    "    #all_story[str(int(sub)+8)] = story\n",
    "    all_story[sub] = story\n",
    "#Fix story sequence\n",
    "#story_sequece = all_story[]\n",
    "#story_sequece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfc48de8-b275-4ec1-9e3c-3f3a96d6eaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/since/Dropbox/RAs/projects/KiKang/Ratings/2.EventRating\n"
     ]
    }
   ],
   "source": [
    "rating_files = \"Ratings/2.EventRating\"\n",
    "fpath = os.path.join(overall_performance_dir, 'KiKang' ,rating_files)\n",
    "print(fpath)\n",
    "fname1 = 'Event_counting_KK1' +'.xlsx'\n",
    "fname2 = 'Event_counting_KK2' +'.xlsx'\n",
    "\n",
    "\n",
    "#sub-01~33\n",
    "use_cols1 = np.arange(3,65)\n",
    "use_cols2 = np.arange(3,67)\n",
    "\n",
    "run = pd.read_excel(os.path.join(fpath, fname1), sheet_name = 'RunningMan',usecols = use_cols1, names = sub_idx)[0:13]\n",
    "run2 = pd.read_excel(os.path.join(fpath, fname1), sheet_name = 'RunningMan2',usecols = use_cols1, names = sub_idx)[0:13]\n",
    "run3 = pd.read_excel(os.path.join(fpath, fname2), sheet_name = 'RunningMan',usecols = use_cols2, names = sub_idx2)[0:13]\n",
    "run4 = pd.read_excel(os.path.join(fpath, fname2), sheet_name = 'RunningMan2',usecols = use_cols2, names = sub_idx2)[0:13]\n",
    "\n",
    "nei = pd.read_excel(os.path.join(fpath, fname1), sheet_name = 'Neighbors',usecols = use_cols1, names = sub_idx)[0:13]\n",
    "nei2 = pd.read_excel(os.path.join(fpath, fname1), sheet_name = 'Neighbors2',usecols = use_cols1, names = sub_idx)[0:13]\n",
    "nei3 = pd.read_excel(os.path.join(fpath, fname2), sheet_name = 'Neighbors',usecols = use_cols2, names = sub_idx2)[0:13]\n",
    "nei4 = pd.read_excel(os.path.join(fpath, fname2), sheet_name = 'Neighbors2',usecols = use_cols2, names = sub_idx2)[0:13]\n",
    "\n",
    "dre = pd.read_excel(os.path.join(fpath, fname1), sheet_name = 'Dream',usecols = use_cols1, names = sub_idx)[0:12]\n",
    "dre2 = pd.read_excel(os.path.join(fpath, fname1), sheet_name = 'Dream2',usecols = use_cols1, names = sub_idx)[0:12]\n",
    "dre3 = pd.read_excel(os.path.join(fpath, fname2), sheet_name = 'Dream',usecols = use_cols2, names = sub_idx2)[0:12]\n",
    "dre4 = pd.read_excel(os.path.join(fpath, fname2), sheet_name = 'Dream2',usecols = use_cols2, names = sub_idx2)[0:12]\n",
    "\n",
    "\n",
    "bli = pd.read_excel(os.path.join(fpath, fname1), sheet_name = 'BlindDate',usecols = use_cols1, names = sub_idx)[0:13]\n",
    "bli2 = pd.read_excel(os.path.join(fpath, fname1), sheet_name = 'BlindDate2',usecols = use_cols1, names = sub_idx)[0:13]\n",
    "bli3 = pd.read_excel(os.path.join(fpath, fname2), sheet_name = 'BlindDate',usecols = use_cols2, names = sub_idx2)[0:13]\n",
    "bli4 = pd.read_excel(os.path.join(fpath, fname2), sheet_name = 'BlindDate2',usecols = use_cols2, names = sub_idx2)[0:13]\n",
    "\n",
    "\n",
    "#df NAN to Zero \n",
    "nei3 = nei3.fillna(0)\n",
    "dre = dre.fillna(0)\n",
    "dre2 = dre2.fillna(0)\n",
    "dre3 = dre3.fillna(0)\n",
    "dre4 = dre4.fillna(0)\n",
    "bli3 = bli3.fillna(0)\n",
    "bli4 = bli4.fillna(0)\n",
    "\n",
    "overall1 = {'run' : run, 'nei':nei, 'dream': dre, 'blind':bli}\n",
    "overall2 = {'run' : run2, 'nei':nei2, 'dream': dre2, 'blind':bli2}\n",
    "overall3 = {'run' : run3, 'nei':nei3, 'dream': dre3, 'blind':bli3}\n",
    "overall4 = {'run' : run4, 'nei':nei4, 'dream': dre4, 'blind':bli4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd2fcdf2-bd69-4cf4-8c42-2dd780fe1d03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "story_onset =  {'run' : [15, 24,36,39,48,63,72,84,96,129,144,162,168], 'nei': [15,24,33,39,51,63,78,90,108,129,141,156,171], \n",
    "                'dream': [15, 27,39,45,63,75,87,102,114,129,144,159] , 'blind': [ 15, 30, 42, 54, 63, 75, 87, 111, 120, 132, 138, 156, 174] }\n",
    "story_offset =  {'run' : [23,35,38,47,62,71,83,95,128,143,161,167,180], 'nei': [23,32,39,50,62,77,89,107,128,140,155,170,180], \n",
    "                 'dream': [26, 38, 44, 62, 74, 86, 101, 113, 128, 143, 158, 165] , 'blind': [29, 41, 53, 62, 74, 86, 110, 119, 131, 137, 155, 173, 180] }\n",
    "\n",
    "storyName = {'RunningMan': 'run', 'BlindDate': 'blind', 'Dream':'dream', 'Neighbors':'nei'}\n",
    "\n",
    "onset = {'Dream1': [0, 165, 345, 525], 'Dream2': [0, 180, 345, 525], 'Dream3': [0, 180, 360, 525], 'Dream4': [0, 180, 345, 540]}\n",
    "\n",
    "\n",
    "sub_recall_dm = {} \n",
    "for sub in sub_sm:\n",
    "    #print(sub)\n",
    " \n",
    "    if sub < 10 : \n",
    "        vis_sub = 'V0'+str(sub)\n",
    "        aud_sub = 'A0'+str(sub)\n",
    "        \n",
    "    else: \n",
    "        vis_sub = 'V'+str(sub)\n",
    "        aud_sub = 'A'+str(sub)\n",
    "    if sub < 34 : \n",
    "        encoding = {'first': overall1, 'second': overall2}\n",
    "    else: \n",
    "        encoding = {'first': overall3, 'second': overall4}\n",
    "    # Generate the events data frame\n",
    "    sequence = np.where(all_story[sub]=='Dream')[0][0]\n",
    "    story_sequence = 'Dream'+str(int(sequence)+1)\n",
    "    timeline = {'first': 0, 'second': 715}\n",
    "    scene_list = []\n",
    "    for story in np.arange(0,4):    \n",
    "        thisStory = storyName[all_story[sub][story]] \n",
    "        for cond in ['first','second']:\n",
    "\n",
    "            for event in range(12):\n",
    "                scene_list.append({\n",
    "                    'onset': timeline[cond] + onset[story_sequence][story] + story_onset[thisStory][event],\n",
    "                    'duration': story_offset[thisStory][event] - story_onset[thisStory][event],\n",
    "                    'modulation' : encoding[cond][thisStory][vis_sub][event],\n",
    "                    'trial_type': f'{cond}_vis'\n",
    "                })\n",
    "                \n",
    "            for event in range(12): \n",
    "                scene_list.append({\n",
    "                    'onset': timeline[cond] +  onset[story_sequence][story] + story_onset[thisStory][event],\n",
    "                    'duration':story_offset[thisStory][event] - story_onset[thisStory][event],\n",
    "                    'modulation' : encoding[cond][thisStory][aud_sub][event],\n",
    "                    'trial_type': f'{cond}_aud'\n",
    "                })\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    scene_events = pd.DataFrame(scene_list)\n",
    "    sub_recall_dm[sub] = scene_events\n",
    "    \n",
    "#np.save('/home/sincerely/jupyterlab/3. Behav/VCNL/results/recall_performances.npy', sub_recall_dm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b08bb395-4eb3-4a96-a32a-09fd999572a6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 38, 39, 40, 42, 43, 44, 45, 46, 48, 49, 51, 52, 54, 55, 57, 58, 59, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76]\n"
     ]
    }
   ],
   "source": [
    "#Subject check \n",
    "data_dir = os.path.join(May_dir, 'data', 'derivatives')\n",
    "file_list = glob.glob(os.path.join(data_dir, 'sub-*', 'func', f'*task-encoding_run-1_space-MNI152NLin6Asym_res-2_desc-brain_mask.nii.gz'))\n",
    "\n",
    "sub_sm =[] \n",
    "for filename in file_list: \n",
    "    sub = os.path.basename(filename).split('_')[0]\n",
    "    sub = os.path.basename(sub).split('-')[1]\n",
    "    sub_sm.append(int(sub)) \n",
    "\n",
    "# sub_sm_exclude = [0] #exclude 9 & 26\n",
    "# sub_sm = np.delete(sub_sm, sub_sm_exclude)\n",
    "print(sub_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ff1d823-e2e9-42f9-9fff-17634b5bc55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 [ 2  3  4  5  6  7  8 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 27\n",
      " 28 29 30 31 32 33 34 35 38 39 40 42 43 44 45 46 48 49 51 52 53 54 55 58\n",
      " 59 64 65 66 67 68 70 71 72 73 74 75 76]\n"
     ]
    }
   ],
   "source": [
    "# Check subjects & conditions \n",
    "sub_sm = np.arange(1,34)\n",
    "sub_sm_exclude = [0, 8, 25] #exclude 1, 9 & 26\n",
    "subjects1 = np.delete(sub_sm, sub_sm_exclude)\n",
    "\n",
    "sub_sm2 = np.arange(34,77)\n",
    "sub_sm_exclude2 = [2, 3, 7, 13, 16, 22, 23, 26, 27,28,29,35] #Excluede 36, 37, 41, 47, 56, 57, 59, 60, 61,62,63,69\n",
    "subjects2 = np.delete(sub_sm2, sub_sm_exclude2)\n",
    "\n",
    "subjects = np.concatenate([subjects1, subjects2])\n",
    "print(len(subjects),subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceed92f-a427-4bc0-89b6-9adbeeef5199",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "A 'modulation' column was found in the given events data and is used.\n",
      "event_size (192, 4)\n",
      "desingMatrix_size (954, 33)\n",
      "Image shape: (91, 109, 91, 954)\n",
      "finish GLM for 4\n",
      "5\n",
      "A 'modulation' column was found in the given events data and is used.\n",
      "event_size (192, 4)\n",
      "desingMatrix_size (954, 33)\n",
      "Image shape: (91, 109, 91, 954)\n"
     ]
    }
   ],
   "source": [
    "# Data Load & Trimming \n",
    "event_types = ['first_vis','first_aud', 'second_vis', 'second_aud'] \n",
    "\n",
    "tr = 1.5  # Repetition time in seconds\n",
    "dummy = 9 \n",
    "\n",
    "sub_sm = [4,5]\n",
    "#load BOLD Volume\n",
    "for sub in sub_sm: \n",
    "#for sub in subjects: \n",
    "\n",
    "    print(sub)\n",
    "    events = pd.DataFrame(sub_recall_dm[sub])\n",
    "    \n",
    "    # Calculate the total number of scans\n",
    "    #total_time = events['onset'].iloc[-1] +  events['duration'].iloc[-1] + dummy  # Time after last event ends\n",
    "    n_scans = 954\n",
    "    frame_times = np.arange(0, n_scans * tr, tr) # Frame times: time for each scan\n",
    "    \n",
    "    # Build a design matrix \n",
    "    design_matrix = make_first_level_design_matrix(frame_times,events,drift_model='cosine')\n",
    "    print('event_size' , events.shape)\n",
    "    print('desingMatrix_size', design_matrix.shape)\n",
    "\n",
    "    # Plot the design matrix\n",
    "    plot_design_matrix(design_matrix)\n",
    "    plot_path = os.path.join(May_dir, 'GLM', 'detail_model', 'dm', f\"sub-{sub}_task-encoding_designMatrix.png\")\n",
    "    plt.savefig(plot_path)\n",
    "\n",
    "    #========================================================================================================\n",
    "    #encoding_file = glob.glob(os.path.join(May_dir,'denoised', f\"sub-{sub}_task-encoding_concatenated.nii.gz\"))\n",
    "    \n",
    "    # encoding_file = glob.glob(os.path.join(May_dir,'GLM','data', f\"sub-{sub}_task-encoding_concatenated.nii.gz\"))\n",
    "    if int(sub) < 10:\n",
    "            file_list1 = os.path.join(May_dir, 'denoised', f\"sub-0{sub}_task-encoding_run-1_4scNsm.nii.gz\")\n",
    "            file_list2 = os.path.join(May_dir, 'denoised', f\"sub-0{sub}_task-encoding_run-2_4scNsm.nii.gz\")\n",
    "    else:\n",
    "        file_list1 = os.path.join(May_dir, 'denoised', f\"sub-{sub}_task-encoding_run-1_4scNsm.nii.gz\")\n",
    "        file_list2 = os.path.join(May_dir, 'denoised', f\"sub-{sub}_task-encoding_run-2_4scNsm.nii.gz\")\n",
    "\n",
    "    # 이미지를 로드하고 두 이미지 연결\n",
    "    image1 = image.load_img(file_list1)\n",
    "    image2 = image.load_img(file_list2)\n",
    "    func_img = concat_imgs([image1, image2])\n",
    "        \n",
    "        \n",
    "    # Load fmri imgs for Fitting     \n",
    "    # print(f'Load concatenated func_img files')\n",
    "    # func_img = image.load_img(encoding_file) \n",
    "    print('Image shape:', func_img.shape)\n",
    "       \n",
    "    glm = FirstLevelModel(t_r=1.5, hrf_model='glover', drift_model='cosine')\n",
    "    glm = glm.fit(func_img, design_matrices=design_matrix)\n",
    "    \n",
    "    for event in event_types: \n",
    "        beta = glm.compute_contrast(event)\n",
    "        beta_img_path = os.path.join(May_dir,'GLM', 'detail_model', 'betas', f\"sub-{sub}_task-encoding_betas_{event}_image.nii.gz\")\n",
    "        nib.save(beta, beta_img_path)\n",
    "    print(f'finish GLM for {sub}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab9f899e-b460-4b7c-9b4d-7ff574f0da55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltools.data import Brain_Data\n",
    "t_stat_imgs = {}\n",
    "t_stat = {}\n",
    "for event in ['first_vis','first_aud', 'second_vis', 'second_aud'] : \n",
    "    con1_file_list = glob.glob(os.path.join(May_dir,'GLM', 'detail_model', 'betas', f\"sub-*_task-encoding_betas_{event}_image.nii.gz\"))\n",
    "    con1_dat = Brain_Data(con1_file_list)\n",
    "    con1_stats = con1_dat.ttest(threshold_dict={'fdr':.05})\n",
    "    t_stat_img = con1_stats['thr_t'].to_nifti()\n",
    "    #t_stat_imgs[event] = t_stat_img\n",
    "    t_stat_imgs[event] =con1_stats['thr_t']\n",
    "\n",
    "    output_dir = os.path.join(May_dir,'GLM', 'detail_model', 'results') \n",
    "    beta_img_path = os.path.join(output_dir, f\"ttested_task-encoding_betas_{event}_fdr_corrected_image.nii.gz\")\n",
    "    nib.save(t_stat_img, beta_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "347acc95-d74e-4e73-91bf-6ebf1e7279e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2  3  4  5  6  7  8 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 27\n",
      " 28 29 30 31 32 33]\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "sub_sm_exclude = [0] #exclude 9 & 26\n",
    "subjects1 = np.delete(subjects1, sub_sm_exclude)\n",
    "print(subjects1)\n",
    "print(len(subjects1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6d5ec00-7cd8-420f-8ec2-0584c30a6aa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sub-01~33\n",
    "sub_sm_exclude = [0] #exclude 9 & 26\n",
    "subjects1 = np.delete(subjects1, sub_sm_exclude)\n",
    "print(subjects1)\n",
    "print(len(subjects1))\n",
    "\n",
    "\n",
    "from nltools.data import Brain_Data\n",
    "vis1_list, aud2_list, vis2_list = [], [], []\n",
    "for sub in subjects1:\n",
    "    vis1 =  os.path.join(May_dir,'GLM', 'detail_model', 'betas', f\"sub-{sub}_task-encoding_betas_first_vis_image.nii.gz\")\n",
    "    vis1_list.append(vis1)\n",
    "    \n",
    "    aud2 = os.path.join(May_dir,'GLM', 'detail_model', 'betas',  f\"sub-{sub}_task-encoding_betas_second_aud_image.nii.gz\")\n",
    "    aud2_list.append(aud2)\n",
    "    \n",
    "    vis2 = os.path.join(May_dir,'GLM', 'detail_model', 'betas',  f\"sub-{sub}_task-encoding_betas_second_vis_image.nii.gz\")\n",
    "    vis2_list.append(vis2)\n",
    "    \n",
    "vis1_dat = Brain_Data(vis1_list)\n",
    "vis2_dat = Brain_Data(vis2_list)\n",
    "aud2_dat = Brain_Data(aud2_list)\n",
    "\n",
    "# con1_stats = vis2_dat - vis1_dat\n",
    "# con1_stats = con1_stats.ttest()\n",
    "\n",
    "# con2_stats = aud2_dat - vis1_dat\n",
    "# con2_stats = con2_stats.ttest()\n",
    "\n",
    "con3_stats = (vis2_dat + vis1_dat)/2\n",
    "con3_stats = con3_stats.ttest()\n",
    "\n",
    "# t_stat_img = con1_stats['t'].to_nifti()\n",
    "# t_stat_img2 = con2_stats['t'].to_nifti()\n",
    "t_stat_img3 = con3_stats['t'].to_nifti()\n",
    "\n",
    "\n",
    "output_dir = os.path.join(May_dir,'GLM', 'detail_model', 'results') \n",
    "#beta_img_path1 = os.path.join(output_dir, f\"ttested_task-encoding_betas_contrast-vis2vis1_uncorrected_image.nii.gz\")\n",
    "#beta_img_path2 = os.path.join(output_dir, f\"ttested_task-encoding_betas_contrast-aud2vis1_uncorrected_image.nii.gz\")\n",
    "beta_img_path3 = os.path.join(output_dir, f\"ttested_task-encoding_betas_mean-vis1vis2_uncorrected_image.nii.gz\")\n",
    "\n",
    "#nib.save(t_stat_img, beta_img_path1)\n",
    "#nib.save(t_stat_img2, beta_img_path2)\n",
    "nib.save(t_stat_img3, beta_img_path3)\n",
    "\n",
    "# con1_stats = vis2_dat - vis1_dat\n",
    "# con1_stats = con1_stats.ttest(threshold_dict={'fdr':.05})\n",
    "# con2_stats = aud2_dat - vis1_dat\n",
    "# con2_stats = con2_stats.ttest(threshold_dict={'fdr':.05})\n",
    "\n",
    "\n",
    "con3_stats = (vis2_dat + vis1_dat)/2\n",
    "con3_stats = con3_stats.ttest(threshold_dict={'fdr':.05})\n",
    "\n",
    "# t_stat_img = con1_stats['thr_t'].to_nifti()\n",
    "# t_stat_img2 = con2_stats['thr_t'].to_nifti()\n",
    "t_stat_img3 = con3_stats['thr_t'].to_nifti()\n",
    "\n",
    "\n",
    "output_dir = os.path.join(May_dir,'GLM', 'detail_model', 'results') \n",
    "# beta_img_path1 = os.path.join(output_dir, f\"ttested_task-encoding_betas_contrast-vis2vis1_fdr_corrected_image.nii.gz\")\n",
    "# beta_img_path2 = os.path.join(output_dir, f\"ttested_task-encoding_betas_contrast-aud2vis1_fdr_corrected_image.nii.gz\")\n",
    "beta_img_path3 = os.path.join(output_dir, f\"ttested_task-encoding_betas_mean-vis1vis2_fdr_corrected_image.nii.gz\")\n",
    "\n",
    "\n",
    "# nib.save(t_stat_img, beta_img_path1)\n",
    "# nib.save(t_stat_img2, beta_img_path2)\n",
    "nib.save(t_stat_img3, beta_img_path3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de4589eb-7698-467c-a294-bb19731158d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34 35 38 39 40 42 43 44 45 46 48 49 51 52 53 54 55 58 59 64 65 66 67 68\n",
      " 70 71 72 73 74 75 76]\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "# Sub-34~76 \n",
    "sub_sm_exclude = [12,] #exclude 50, 73 \n",
    "subjects2 = np.delete(subjects2, sub_sm_exclude)\n",
    "print(subjects2)\n",
    "print(len(subjects2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dc0f51f-9159-4f58-96e9-1dfaf99e9bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sub-34~76\n",
    "sub_sm_exclude = [12,] #exclude 50, 73 \n",
    "subjects2 = np.delete(subjects2, sub_sm_exclude)\n",
    "print(subjects2)\n",
    "print(len(subjects2))\n",
    "\n",
    "aud1_list, aud2_list, vis2_list = [], [], []\n",
    "for sub in subjects2:\n",
    "    aud1 =  os.path.join(May_dir,'GLM', 'detail_model', 'betas', f\"sub-{sub}_task-encoding_betas_first_aud_image.nii.gz\")\n",
    "    aud1_list.append(aud1)\n",
    "    \n",
    "    aud2 = os.path.join(May_dir,'GLM', 'detail_model', 'betas',  f\"sub-{sub}_task-encoding_betas_second_aud_image.nii.gz\")\n",
    "    aud2_list.append(aud2)\n",
    "    \n",
    "    vis2 = os.path.join(May_dir,'GLM', 'detail_model', 'betas',  f\"sub-{sub}_task-encoding_betas_second_vis_image.nii.gz\")\n",
    "    vis2_list.append(vis2)\n",
    "    \n",
    "aud1_dat = Brain_Data(aud1_list)\n",
    "vis2_dat = Brain_Data(vis2_list)\n",
    "aud2_dat = Brain_Data(aud2_list)\n",
    "\n",
    "# print('first contrast')\n",
    "# con1_stats = aud2_dat - aud1_dat\n",
    "# con1_stats = con1_stats.ttest()\n",
    "# con2_stats = vis2_dat -aud1_dat\n",
    "# con2_stats = con2_stats.ttest()\n",
    "\n",
    "\n",
    "# t_stat_img = con1_stats['t'].to_nifti()\n",
    "# t_stat_img2 = con2_stats['t'].to_nifti()\n",
    "\n",
    "\n",
    "con3_stats = (aud1_dat + aud2_dat)/2\n",
    "con3_stats = con3_stats.ttest()\n",
    "\n",
    "# t_stat_img = con1_stats['t'].to_nifti()\n",
    "# t_stat_img2 = con2_stats['t'].to_nifti()\n",
    "t_stat_img3 = con3_stats['t'].to_nifti()\n",
    "\n",
    "\n",
    "output_dir = os.path.join(May_dir,'GLM', 'detail_model', 'results') \n",
    "# beta_img_path1 = os.path.join(output_dir, f\"ttested_task-encoding_betas_contrast-aud2aud1_uncorrected_image.nii.gz\")\n",
    "# beta_img_path2 = os.path.join(output_dir, f\"ttested_task-encoding_betas_contrast-vis2aud1_uncorrected_image.nii.gz\")\n",
    "beta_img_path3 = os.path.join(output_dir, f\"ttested_task-encoding_betas_mean-aud1aud2_uncorrected_image.nii.gz\")\n",
    "\n",
    "\n",
    "# nib.save(t_stat_img, beta_img_path1)\n",
    "# nib.save(t_stat_img2, beta_img_path2)\n",
    "nib.save(t_stat_img3, beta_img_path3)\n",
    "\n",
    "\n",
    "# con1_stats = aud2_dat - aud1_dat\n",
    "# con1_stats = con1_stats.ttest(threshold_dict={'fdr':.05})\n",
    "# con2_stats = vis2_dat -aud1_dat\n",
    "# con2_stats = con2_stats.ttest(threshold_dict={'fdr':.05})\n",
    "con3_stats = (aud1_dat + aud2_dat)/2\n",
    "con3_stats = con3_stats.ttest(threshold_dict={'fdr':.05})\n",
    "\n",
    "# t_stat_img = con1_stats['thr_t'].to_nifti()\n",
    "# t_stat_img2 = con2_stats['thr_t'].to_nifti()\n",
    "t_stat_img3 = con3_stats['thr_t'].to_nifti()\n",
    "\n",
    "\n",
    "# output_dir = os.path.join(May_dir,'GLM', 'detail_model', 'results') \n",
    "# beta_img_path1 = os.path.join(output_dir, f\"ttested_task-encoding_betas_contrast-aud2aud1_fdr_corrected_image.nii.gz\")\n",
    "# beta_img_path2 = os.path.join(output_dir, f\"ttested_task-encoding_betas_contrast-vis2aud1_fdr_corrected_image.nii.gz\")\n",
    "beta_img_path3 = os.path.join(output_dir, f\"ttested_task-encoding_betas_mean-aud1aud2_fdr_corrected_image.nii.gz\")\n",
    "\n",
    "# nib.save(t_stat_img, beta_img_path1)\n",
    "# nib.save(t_stat_img2, beta_img_path2)\n",
    "nib.save(t_stat_img3, beta_img_path3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "391f850a-1f90-4144-ac84-6c22368e8d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conjunctive map \n",
    "import os\n",
    "import glob\n",
    "import nibabel as nib\n",
    "from nilearn.masking import compute_multi_epi_mask\n",
    "\n",
    "# Load file paths\n",
    "file_list = []\n",
    "for file in ['vis2aud1_uncorrected', 'aud2vis1_uncorrected']:\n",
    "    file_dir = glob.glob(os.path.join(May_dir, 'GLM', 'detail_model', 'results', f'*{file}*.nii.gz'))\n",
    "    if file_dir:\n",
    "        file_list.append(file_dir[0])  # Append file path instead of loading data\n",
    "\n",
    "# Compute the mask\n",
    "mask_img = compute_multi_epi_mask(file_list)  # Pass file paths to compute_multi_epi_mask\n",
    "\n",
    "# Save the resulting mask\n",
    "nib.save(mask_img, os.path.join(May_dir, 'GLM', 'detail_model', 'results', 'conjunctive_map_btw_conditions.nii.gz'))\n",
    "\n",
    "\n",
    "# ㅇㅏ ㅁㅓㄴㄱㅏ ㅇㅣㅅㅏㅇㅎㅏㅁ.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72db8d36-4faf-4eda-af11-e1d4eff75f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fmri01",
   "language": "python",
   "name": "fmri01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
